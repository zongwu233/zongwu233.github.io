<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="HandheldFriendly" content="True">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="referrer" content="no-referrer-when-downgrade">

        <title>大语言模型 (Large Language Model)涌现能力随想</title>
        <meta name="description" content="">

        <link rel="stylesheet" href="https://blog.zongwu233.xyz/main.css">

        
        <link rel="alternate" type="application/rss+xml" title="RSS" href="https://blog.zongwu233.xyz/rss.xml">
        

        
        
        
        
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-68155231-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'UA-68155231-1');
        </script>

        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZF40590PZW"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
              gtag('js', new Date());

            gtag('config', 'G-ZF40590PZW');
        </script>
        
    </head>
    <body>

      <a class="skip-main" href="#main">Skip to content</a>
        <div class="container">
            <header> 
                <h1 class="site-header">
                    <a href="https:&#x2F;&#x2F;blog.zongwu233.xyz">zongwu&#x27;s blog</a>
                </h1>
                <nav>
                    
                    
                    
                    <a  href="https:&#x2F;&#x2F;blog.zongwu233.xyz">Home</a>
                    
                    
                    <a  href="https:&#x2F;&#x2F;blog.zongwu233.xyz&#x2F;categories&#x2F;">Categories</a>
                    
                    
                    <a  href="https:&#x2F;&#x2F;blog.zongwu233.xyz&#x2F;tags&#x2F;">Tags</a>
                    
                    
                    <a  href="https:&#x2F;&#x2F;blog.zongwu233.xyz&#x2F;about&#x2F;">About</a>
                    
                    
                    <a  href="https:&#x2F;&#x2F;blog.zongwu233.xyz&#x2F;rss.xml">RSS</a>
                    
                    
                </nav>
            </header>
            <main id="main" tabindex="-1">
                

<article class="post">
    <header>
        <h1>大语言模型 (Large Language Model)涌现能力随想</h1>
    </header>
    <div class="content">
        <p>科学技术以及工业界主流思维范式依然是还原论。</p>
<p>所谓还原论简单讲就是，我们坚信所有的系统都是可以通过合适的拆分，一步步分解成简单的构件，逐个认知/实现所有的构件并组装起来，最终就理解/实现了原来的系统。</p>
<p>无论是量子物理还是国家税收系统，无论是复杂的航天工程还是小商贩日常使用的电子秤，无论是发明几百万人使用的编程语言还是小公司试图制定新的年度计划，我们都坚信事物总是由其子模块构成的，通过分而治之的办法我们总能构建出来某些事物。</p>
<p>数学家、哲学家、现代还原论的标志性人物笛卡尔在其《方法论》中提出了还原论方法的基本原则：</p>
<blockquote>
<p>把我所考察的每一个难题，都尽可能地分成细小的部分，直到可以适于加以圆满解决的程度为止</p>
</blockquote>
<p>回溯以牛顿力学的出现为代表的自然科学的开端，可以看到整个自然科学的发展都是建立在还原论基础之上。物理化学生物医药材料信息等等学科的还原式分化确立与繁荣的过程，都不断地印证并强化着人类对还原论这一哲思的信念。</p>
<span id="continue-reading"></span>
<p>那个站在20世纪之交希望为所有的数学理论构建统一的最底层的公理体系来容纳所有数学研究对象和推理的伟大数学家希尔伯特，面对依然未解决的重大数学问题，回应怀疑论者和不可知论者：</p>
<blockquote>
<p>We must know. We will know.</p>
</blockquote>
<p>从若干显而易见的概念和尽可能少的几条公理出发，利用纯逻辑推理，尽可能多地推导出领域中的定理，数学家们这种公理化方法的背后依然是还原论：</p>
<p>任何复杂的定理或者推论，总是可以由若干简单的概念和公理构建（遵循逻辑推理规则推理）出来。</p>
<p>希尔伯特的公理体系设想很快被哥德尔证明是无法实现完备性的（但这并不代表着还原论全面败给了不可知论），经典力学遇到如何解释光速这个棘手问题，还原论终将遇到涌现（emergence）现象 ，之前大杀四方的理论和技术手段都黯然失色无法从容应对。</p>
<p>还原论一直无法自洽地从逻辑上解释涌现现象。</p>
<p>由简单个体蚂蚁构成的蚁群是如何表现出了相当高的智能性？
庞大的鸟群聚集一起飞翔的时候，没有任何中心化的指挥，但犹如一个巨大的生物灵活自如行动，其中任何两只鸟并不会碰撞。
当然最最迷人的问题是人类的大脑，拥有上千亿个神经元的大脑，是如何进行学习、思考、遗忘抑或是每时每刻都在冒出想法的？即便是我们了解了神经元的结构：树突、轴突、神经递质等等，我们依然无法解释/模仿人类大脑如何产生智能的机制。</p>
<p>还原论被卡住了（Reductionism is stuck）。</p>
<p>以还原论为认知基础的自然科学实际上也卡住了。自20世纪70年代至21世纪的这几十年里，再也没有自然科学领域重大的突破性新发现、没有基础科学领域颠覆性的新学说。有的只是这些学说和发现在各个领域的具体应用和技术改良。</p>
<p>从理性出发，在认知探索世界的道路上人类取得了无数的辉煌成就，但是这几十年的技术大停滞，是否意味着还原论走到了能力极限？</p>
<p>直到 OpenAI 构建的 LLM 大模型表现出了涌现能力。以研究涌现为核心的复杂性科学或许将迎来科学范式革命的微弱的黎明曙光。</p>
<p>现代机器学习的发端是模仿人类神经网络的原理，其结果的不可解释性和不稳定性一直备受诟病，激进的/正统的还原主义者认为机学习是歧途，甚至都不能称之为数据科学。但正是这个与主流还原论思想格格不入的技术，几经沉沦，近十年来得益于互联网海量可用数据以及并行计算硬件的成本降低，一路高歌猛进并不断地从落地应用层面给人们带来意外与惊喜。LLM 大模型这个当下机器学习领域的 state-of-the-art ，因其至少十亿参数（下一代可能会是万亿）的规模而知名，除了完成常规的机器学习的任务，还表现出了涌现能力。</p>
<p>在此之前，人类早就能够构建具备涌现能力的系统，比如元胞自动机(cellular automata，CA)，通过运行一定的时间，元细胞自动机就能涌现出某种神秘的秩序。互联网、人类经济活动也都是具备涌现现象的系统。甚至当你骑着自行车平稳向前运动，都属于一种涌现现象（目前自然科学无法精确给出这种稳定态描述公式或者方程，很惊奇吧）。凭什么 LLM的涌现现象将是革命性的影响？</p>
<p>因为以往人类观察或者构建的具备涌现现象的系统，要么无法直接应用，只存在于理论层面，要么不具备通用性，仅仅能适合某个特定场景。</p>
<p>此外，在此之前的的人工智能技术与其归属于人工智能领域不如说是归属于机械与自动化领域。因为它们始终通向人类直接或者间接的意图/指令：</p>
<p>人工定义规则/专家系统，实质是关键字（模糊）匹配。
动态规划，让系统递归地自动匹配规则。
经典机器学习（深度学习之前的机器学习），投喂大量的数据和或配合监督训练，让机器先学会挖掘隐式规则再应用其分类能力。
这些始终都未能超出自动化控制理论的范围。</p>
<p>LLM大模型有什么不同？到底神奇在什么地方？给大模型一些少量的任务提示，大模型就能执行未见过的（从未学习过的）推理任务。这种涌现能力跨越各种语言模型、任务类型和实验场景。这种仅在大规模模型才出现的涌现能力的存在也意味着，随着参数规模的扩张，可以进一步增强LLM的能力（很好奇具备了与人类大脑神经元个数相同规模参数的GPT4将会体现怎样的智能）。</p>
<p>LLM大模型标志着人类制造出了通用的、非预定义的、非直接意图的智能。</p>
<p>这种通用性跨越行业的限制，不需要针对每一个特定领域进行训练模型，而是具备广泛的行业应用的可能性。这种非预定性和非直接意图，不再要求模型的使用者具备调整参数的能力，也即意味着大众可以从直接应用这个模型中受益。ChatGPT产品收到的各行业的积极反馈，已经暗示着LLM大模型将是一个能够在多个行业带来深刻变革的基础性技术变革。</p>
<p>除了具体的技术的应用，更让人期待的是，让我们等待复杂性科学领域，牛顿式人物的降临。在那一刻，我们将获得一个简洁的公式描述复杂性。</p>

    </div>

    
    <div class="article-info">
        
        <div class="article-date">2023-02-27</div>
        
        <div class="article-taxonomies">
            
                <ul class="article-categories">
                    
                    <li><a href="https://blog.zongwu233.xyz/categories/mental/">mental</a></li>
                    
                </ul>
            
            
        </div>
    </div>

</article>


            </main>
            <footer>
                <p>
                © zongwu&#x27;s blog 2015 - 2023<br>
                </p>
                <p>
                
                
                </p>
            </footer>
        </div>
</body>
</html>
